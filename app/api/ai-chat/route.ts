import { type NextRequest, NextResponse } from "next/server"

export async function POST(request: NextRequest) {
  try {
    const { message, budget, config } = await request.json()

    // Verificar se a API Key est√° configurada no Vercel
    const apiKey = process.env.OPENAI_API_KEY
    if (!apiKey) {
      console.error("‚ùå OPENAI_API_KEY n√£o configurada no Vercel")
      return NextResponse.json({ error: "API Key da OpenAI n√£o configurada no servidor" }, { status: 500 })
    }

    console.log("ü§ñ Processando requisi√ß√£o para IA:", {
      hasMessage: !!message,
      hasBudget: !!budget,
      model: config?.model || "n√£o especificado",
      temperature: config?.temperature || "n√£o especificado",
    })

    // Preparar contexto baseado no or√ßamento (se fornecido)
    let contextualMessage = message
    if (budget) {
      const budgetContext = `
Dados do Or√ßamento:
- Sequ√™ncia: ${budget.sequencia_orcamento || "N/A"}
- Cliente: ${budget.nome_cliente || "N/A"}
- Vendedor: ${budget.nome_vendedor || "N/A"}
- Valor: R$ ${budget.valor_orcamento || "N/A"}
- Data: ${budget.data_orcamento || "N/A"}
- Status: ${budget.status || "N/A"}
- Dias desde cria√ß√£o: ${budget.dias_desde_criacao || "N/A"}
- Observa√ß√µes: ${budget.observacoes || "Nenhuma"}

Pergunta: ${message}
`
      contextualMessage = budgetContext
    }

    // Fazer requisi√ß√£o para OpenAI
    const openaiResponse = await fetch("https://api.openai.com/v1/chat/completions", {
      method: "POST",
      headers: {
        Authorization: `Bearer ${apiKey}`,
        "Content-Type": "application/json",
      },
      body: JSON.stringify({
        model: config?.model || "gpt-4o-mini",
        messages: [
          {
            role: "system",
            content:
              config?.systemPrompt ||
              "Voc√™ √© um assistente especializado em vendas e follow-up de or√ßamentos. Seja profissional, objetivo e √∫til.",
          },
          {
            role: "user",
            content: contextualMessage,
          },
        ],
        temperature: config?.temperature || 0.7,
        max_tokens: config?.maxTokens || 1000,
      }),
    })

    if (!openaiResponse.ok) {
      const errorData = await openaiResponse.json()
      console.error("‚ùå Erro da OpenAI:", errorData)
      return NextResponse.json(
        { error: errorData.error?.message || "Erro na API da OpenAI" },
        { status: openaiResponse.status },
      )
    }

    const data = await openaiResponse.json()
    const aiResponse = data.choices[0]?.message?.content || "Desculpe, n√£o consegui gerar uma resposta."

    console.log("‚úÖ Resposta da IA gerada com sucesso")

    return NextResponse.json({
      response: aiResponse,
      model: config?.model || "gpt-4o-mini",
      usage: data.usage,
    })
  } catch (error: any) {
    console.error("‚ùå Erro no processamento da IA:", error)
    return NextResponse.json({ error: `Erro interno: ${error.message}` }, { status: 500 })
  }
}
