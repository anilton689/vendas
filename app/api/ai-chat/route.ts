import { type NextRequest, NextResponse } from "next/server"

export async function POST(request: NextRequest) {
  try {
    const { message, budget, config } = await request.json()

    // Verificar se a API Key est√° configurada no Vercel
    const apiKey = process.env.OPENAI_API_KEY
    if (!apiKey) {
      console.error("‚ùå OPENAI_API_KEY n√£o configurada no Vercel")
      return NextResponse.json({ error: "API Key da OpenAI n√£o configurada no servidor" }, { status: 500 })
    }

    console.log("ü§ñ [AI-API] Processando requisi√ß√£o:", {
      hasMessage: !!message,
      messageLength: message?.length || 0,
      hasBudget: !!budget,
      hasConfig: !!config,
      model: config?.model || "n√£o especificado",
      systemPromptLength: config?.systemPrompt?.length || 0,
      systemPromptValue: config?.systemPrompt ? "presente" : "ausente",
    })

    // Validar se a mensagem n√£o est√° vazia
    if (!message || typeof message !== "string" || message.trim().length === 0) {
      console.error("‚ùå [AI-API] Mensagem inv√°lida:", { message, type: typeof message })
      return NextResponse.json({ error: "Mensagem √© obrigat√≥ria e deve ser uma string v√°lida" }, { status: 400 })
    }

    // Preparar contexto baseado no or√ßamento (se fornecido)
    let contextualMessage = message.trim()
    if (budget) {
      const budgetContext = `
DADOS DO OR√áAMENTO PARA AN√ÅLISE:
- Sequ√™ncia: ${budget.sequencia_orcamento || "N/A"}
- Cliente: ${budget.nome_cliente || "N/A"}
- Vendedor: ${budget.nome_vendedor || "N/A"}
- Valor: R$ ${budget.valor_orcamento?.toLocaleString("pt-BR") || "N/A"}
- Data: ${budget.data_orcamento || "N/A"}
- Status: ${budget.status || "N/A"}
- Dias desde cria√ß√£o: ${budget.dias_desde_criacao || "N/A"}
- Observa√ß√µes: ${budget.observacoes || "Nenhuma"}

PERGUNTA DO USU√ÅRIO: ${message.trim()}
`
      contextualMessage = budgetContext
    }

    // Usar o systemPrompt da configura√ß√£o com fallback seguro
    let systemPrompt =
      "Voc√™ √© um assistente especializado em vendas e follow-up de or√ßamentos. Seja profissional, objetivo e √∫til."

    if (config?.systemPrompt && typeof config.systemPrompt === "string" && config.systemPrompt.trim().length > 0) {
      systemPrompt = config.systemPrompt.trim()
      console.log("‚úÖ [AI-API] Usando systemPrompt personalizado:", systemPrompt.substring(0, 100) + "...")
    } else {
      console.log("‚ö†Ô∏è [AI-API] Usando systemPrompt padr√£o (config inv√°lido):", {
        hasConfig: !!config,
        hasSystemPrompt: !!config?.systemPrompt,
        systemPromptType: typeof config?.systemPrompt,
        systemPromptLength: config?.systemPrompt?.length || 0,
      })
    }

    // Validar que o systemPrompt n√£o √© null/undefined
    if (!systemPrompt || typeof systemPrompt !== "string") {
      console.error("‚ùå [AI-API] SystemPrompt inv√°lido:", { systemPrompt, type: typeof systemPrompt })
      systemPrompt =
        "Voc√™ √© um assistente especializado em vendas e follow-up de or√ßamentos. Seja profissional, objetivo e √∫til."
    }

    // Validar que a mensagem contextual n√£o √© null/undefined
    if (!contextualMessage || typeof contextualMessage !== "string") {
      console.error("‚ùå [AI-API] Mensagem contextual inv√°lida:", { contextualMessage, type: typeof contextualMessage })
      contextualMessage = message.trim() || "Como posso ajudar?"
    }

    console.log("üìù [AI-API] Dados finais para OpenAI:", {
      systemPromptLength: systemPrompt.length,
      messageLength: contextualMessage.length,
      model: config?.model || "gpt-4o-mini",
      temperature: config?.temperature || 0.7,
      maxTokens: config?.maxTokens || 1000,
    })

    // Fazer requisi√ß√£o para OpenAI
    const openaiResponse = await fetch("https://api.openai.com/v1/chat/completions", {
      method: "POST",
      headers: {
        Authorization: `Bearer ${apiKey}`,
        "Content-Type": "application/json",
      },
      body: JSON.stringify({
        model: config?.model || "gpt-4o-mini",
        messages: [
          {
            role: "system",
            content: systemPrompt, // Garantido que √© string v√°lida
          },
          {
            role: "user",
            content: contextualMessage, // Garantido que √© string v√°lida
          },
        ],
        temperature: config?.temperature || 0.7,
        max_tokens: config?.maxTokens || 1000,
      }),
    })

    console.log("üì° [AI-API] Status da resposta OpenAI:", openaiResponse.status)

    if (!openaiResponse.ok) {
      const errorData = await openaiResponse.json()
      console.error("‚ùå [AI-API] Erro da OpenAI:", errorData)
      return NextResponse.json(
        { error: errorData.error?.message || "Erro na API da OpenAI" },
        { status: openaiResponse.status },
      )
    }

    const data = await openaiResponse.json()
    const aiResponse = data.choices[0]?.message?.content || "Desculpe, n√£o consegui gerar uma resposta."

    console.log("‚úÖ [AI-API] Resposta gerada com sucesso:", {
      responseLength: aiResponse.length,
      usage: data.usage,
    })

    return NextResponse.json({
      response: aiResponse,
      model: config?.model || "gpt-4o-mini",
      usage: data.usage,
    })
  } catch (error: any) {
    console.error("‚ùå [AI-API] Erro no processamento:", error)
    return NextResponse.json({ error: `Erro interno: ${error.message}` }, { status: 500 })
  }
}
